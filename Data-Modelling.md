# Data Modelling

## Last Resort for conceptualizing Data Model regardless of Database Vendor

*Use the property-graph model*.

By property-graph model, I am referring to the near equivalent of entity-relational model in which both nodes and edges are organised into bags/multisets, distinction exists between nodes and edges/relationships, and distinction exists between one-sided relationships which cannot have properties, and all other kind of relationships.
    1. have a way to identify the type of each node, and each edge (ie either one-side relationship which cannot have properties, or not). And in the case of edges, also have a way to identify the targets of the relationship.
    1. abstract all relationships as potentially many-to-many to the public, i.e. hide one-sided relationship implementation details from the public.
    2. differentiate between the means of distinguishing entities for the purpose of establishing relationships in the property-graph model, from all other criteria for distinguishing entities, including criteria known to the public
        - E.g. always use internally-generated ids to identify entities for all programming purposes, including for forming relationships and for presenting to the public.

## Last Resort Options for achieving ACID

"ACID" here refers to Atomicity, Consistency, Isolation, Durability

  1. "ACID" databases
     - best at achieving durability.
     - best at achieving atomicty by abortability.
     - best at achieving snapshot isolation for readonly queries.
     - best at preventing access to results from uncommitted transactions.
     - insufficient and inflexible in achieving consistency by themselves.
  2. Applications and business rules
     - last resort for consistency.
  1. Try-Confirm-Cancel, mentioned by Pat Helland in 2007.
     - achieves isolation without forcing sequential processing.
  2. Log-based message broker
     - last resort for isolation, in which messages are processed one at a time, effectively forcing sequential processing, and hence achieving isolation.
     - last resort for atomicity by endless retrying

## Last Resort for Querying regardless of Database or Model

*Break querying into stages, consider involving application code to query the SQL/NoSQL query results, and consider involving repeat querying.*
  - By repeat querying, I mean: save either query result set or dataset from a data source, into embedded SQL databases (such as SQLite and DuckDB) or temporary SQL tables of popular RDBMses, query them in SQL, consider involving application code to query the SQL query results, save the result set to SQL tables, and repeat the querying and saving until the desired results are obtained.
  
This approach provides one solution to the problem of leveraging query languages and also having a way to guarantee that all querying code snippets generated by program at runtime are correct. And that solution is to use permutations and combinations to generate and test all the possibilities of the dynamically generated querying code snippets, as long as variations are kept below a limit (say 7 permutations). Then further variations can be dealt with by involving application code querying.

The application-code side of querying should resemble MapReduce, ie is based on reduce (e.g. map, filter, selectMany, groupByAdjacent, application of window functions), sort, merge-join (e.g. union, intersection, except, equi-join), and functions common to most
databases and programming languages (e.g. LIKE, uppercase, arithmetic).

Repeat querying can serve as a last resort for traversing relationships in any database model, since traversing relationships is arguably equivalent to performing merge-joins.

Some possible dependencies:
  1. May require easy import or export of large datasets into/from temporary databases or tables, and the automatic deletion of such temporary objects after some timeouts.
  2. May have to deal with expectation of SQL table schema, by dealing with issues like
     - automatically identifiying column types (and even names) in result sets of queries
     - creating temporary tables based on schema suggested by column types to store such query results
     - converting result sets into typed record list

## Minimal ORM Requirements for both SQL and NoSQL Databases

  1. Institute custom code generator for abstracting names and types of SQL table columns, and names of other database objects (i.e. tables, sequences, stored procedures, etc).
  1. Implement function for converting between trees and record list
     - split function into (1) mapping record list into tuples of classes and (2) map tuples of classes to trees (e.g. using groupByAdjacent operator on sorted record list, such as can be found in morelinq library) and vice versa (e.g. using selectMany operator such as can be found in Reactive Extensions).
     - main job then is about mapping result set to list of tuples of "field/column sets", where each column set doesn't duplicate field names, but a name may be present in two or more column sets. 
     - So given a row like {name:A, value:1}, {name:B, value:2}, {name:A, value: 0}, and a list of column sets like class C1 {A,B}, class C2 {A}, the row should be mapped to something like C1(A=1,B=2), C2(A=0).
     - application-defined mapper functions can then convert the database classes used in the tuples, to classes with property names which are more convenient to higher layers  of an application.
  2. Implement function for loading targets of many to many relationships, given id
     - implement efficiently in document db, sql or graph db using knowledge of all distinct ordered ids whose targets are to be loaded. By having many to many table sorted by source ids, fetch target ids, and use WHERE id in (target ids)
  3. Deserialization logic of query results (e.g. mappers between query results and data transfer objects) may need to be maintained with production data.
     - SQL makes this considerably easier than NoSQL, because even an empty query result set can sufficiently indicate the columns/fields of the query result.
     - Generally however for both SQL and NoSQL, existing database contents define the schema of database contents. So during production one can track the schemas of query results, and make this data available during regression testing of changes to mapping logic.
     - Then once this data is made available to mapper tests, all they have to check for is whether their fields of interest are found, and additionally are of the expected types.

Learn from the following before attempting to implement advanced ORM features, such as abstracting names of SQL table columns, and fetching target of relationships.
  - https://blog.codinghorror.com/object-relational-mapping-is-the-vietnam-of-computer-science/
  - https://scala-slick.org/doc/3.0.0/orm-to-slick.html
